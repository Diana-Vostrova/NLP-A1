{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "When solving this task, we expect you'll face (and successfully deal with) some problems or make up the ideas of the model improvement. Some of them are: \n",
        "\n",
        "\n",
        "- solving a problem of n-grams frequencies storing for a large corpus;\n",
        "- taking into account keyboard layout and associated misspellings;\n",
        "- efficiency improvement to make the solution faster;\n",
        "- ...\n",
        "\n",
        "Please don't forget to describe such cases, and what you decided to do with them, in the Justification section.\n",
        "\n",
        "##### IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "MoQeEsZvHvvi"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import re\n",
        "import heapq\n",
        "from collections import Counter, defaultdict\n",
        "import math\n",
        "import re\n",
        "import heapq\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "class SpellCorrector:\n",
        "    def __init__(self, corpus_path=\"big.txt\", fivegram_path=\"fivegrams.txt\"):\n",
        "        self.unigrams = Counter()\n",
        "        self.bigrams = defaultdict(Counter)\n",
        "        self.total_words = 0\n",
        "        self._load_corpus(corpus_path)\n",
        "        self.fivegrams = {}\n",
        "        self.fivegram_prefix = defaultdict(int)\n",
        "        self._load_fivegrams(fivegram_path)\n",
        "\n",
        "    def _load_corpus(self, path):\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read().lower()\n",
        "        words = re.findall(r\"[a-z]+\", text)\n",
        "        self.unigrams = Counter(words)\n",
        "        self.total_words = sum(self.unigrams.values())\n",
        "        for i in range(len(words)-1):\n",
        "            self.bigrams[words[i]][words[i+1]] += 1\n",
        "\n",
        "    def _load_fivegrams(self, path):\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) != 6:\n",
        "                    continue \n",
        "                try:\n",
        "                    count = int(parts[5])\n",
        "                except ValueError:\n",
        "                    continue\n",
        "                fivegram = tuple(parts[:5])\n",
        "                self.fivegrams[fivegram] = count\n",
        "                prefix = (fivegram[0], fivegram[1], fivegram[3], fivegram[4])\n",
        "                self.fivegram_prefix[prefix] += count\n",
        "\n",
        "    def edits1(self, word):\n",
        "        letters = \"abcdefghijklmnopqrstuvwxyz\"\n",
        "        splits = [(word[:i], word[i:]) for i in range(len(word)+1)]\n",
        "        deletes = [L + R[1:] for L, R in splits if R]\n",
        "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
        "        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
        "        inserts = [L + c + R for L, R in splits for c in letters]\n",
        "        return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "    def edits2(self, word):\n",
        "        return {e2 for e1 in self.edits1(word) for e2 in self.edits1(e1)}\n",
        "\n",
        "    def known(self, words):\n",
        "        return set(w for w in words if w in self.unigrams)\n",
        "\n",
        "    def candidates(self, word):\n",
        "        if word in self.unigrams:\n",
        "            return {word}\n",
        "        cand = self.known(self.edits1(word))\n",
        "        if cand:\n",
        "            return cand\n",
        "        cand = self.known(self.edits2(word))\n",
        "        if cand:\n",
        "            return cand\n",
        "        return {word}\n",
        "\n",
        "    def edit_distance(self, a, b):\n",
        "        m, n = len(a), len(b)\n",
        "        dp = [[0]*(n+1) for _ in range(m+1)]\n",
        "        for i in range(m+1):\n",
        "            dp[i][0] = i\n",
        "        for j in range(n+1):\n",
        "            dp[0][j] = j\n",
        "        for i in range(1, m+1):\n",
        "            for j in range(1, n+1):\n",
        "                if a[i-1] == b[j-1]:\n",
        "                    dp[i][j] = dp[i-1][j-1]\n",
        "                else:\n",
        "                    dp[i][j] = 1 + min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])\n",
        "        return dp[m][n]\n",
        "\n",
        "    def score_candidate(self, orig_word, candidate, left_context, right_context, penalty_weight=1.0):\n",
        "        score = 0.0\n",
        "        uni_prob = (self.unigrams[candidate] + 1) / (self.total_words + len(self.unigrams))\n",
        "        score += math.log(uni_prob)\n",
        "        \n",
        "        if left_context:\n",
        "            left = left_context[-1]\n",
        "            bi_count = self.bigrams[left][candidate]\n",
        "            total_left = sum(self.bigrams[left].values())\n",
        "            bi_prob = (bi_count + 1) / (total_left + len(self.unigrams))\n",
        "            score += math.log(bi_prob)\n",
        "        \n",
        "        if right_context:\n",
        "            right = right_context[0]\n",
        "            bi_count = self.bigrams[candidate][right]\n",
        "            total_candidate = sum(self.bigrams[candidate].values())\n",
        "            ri_prob = (bi_count + 1) / (total_candidate + len(self.unigrams))\n",
        "            score += math.log(ri_prob)\n",
        "        \n",
        "        if len(left_context) >= 2 and len(right_context) >= 2:\n",
        "            left2, left1 = left_context[-2], left_context[-1]\n",
        "            right1, right2 = right_context[0], right_context[1]\n",
        "            key = (left2, left1, candidate, right1, right2)\n",
        "            count_5g = self.fivegrams.get(key, 0)\n",
        "            prefix = (left2, left1, right1, right2)\n",
        "            total_prefix = self.fivegram_prefix.get(prefix, 0)\n",
        "            five_prob = (count_5g + 1) / (total_prefix + len(self.unigrams))\n",
        "            score += math.log(five_prob)\n",
        "        \n",
        "        ed = self.edit_distance(orig_word, candidate)\n",
        "        score -= penalty_weight * ed\n",
        "        \n",
        "        return score\n",
        "\n",
        "    def correct_word(self, word, left_context=[], right_context=[]):\n",
        "        best_candidate = word\n",
        "        best_score = -float('inf')\n",
        "        for cand in self.candidates(word):\n",
        "            sc = self.score_candidate(word, cand, left_context, right_context)\n",
        "            if sc > best_score:\n",
        "                best_score = sc\n",
        "                best_candidate = cand\n",
        "        return best_candidate\n",
        "\n",
        "    def correct_sentence(self, sentence, beam_width=5):\n",
        "        tokens = sentence.split()\n",
        "        beam = [(0.0, tokens)]\n",
        "        for i in range(len(tokens)):\n",
        "            new_beam = []\n",
        "            for cumulative_score, seq in beam:\n",
        "                word = seq[i]\n",
        "                for cand in self.candidates(word):\n",
        "                    new_seq = list(seq)\n",
        "                    new_seq[i] = cand\n",
        "                    new_score = 0.0\n",
        "                    for j, w in enumerate(new_seq):\n",
        "                        left = new_seq[:j]\n",
        "                        right = new_seq[j+1:]\n",
        "                        new_score += self.score_candidate(tokens[j], w, left, right)\n",
        "                    heapq.heappush(new_beam, (new_score, new_seq))\n",
        "            beam = heapq.nlargest(beam_width, new_beam, key=lambda x: x[0])\n",
        "        best_score, best_seq = max(beam, key=lambda x: x[0])\n",
        "        return \" \".join(best_seq)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    corpus_file = \"big.txt\"         \n",
        "    fivegram_file = \"fivegrams.txt\"\n",
        "\n",
        "    corrector = SpellCorrector(corpus_path=corpus_file, fivegram_path=fivegram_file)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oML-5sJwGRLE"
      },
      "source": [
        "## Justify your decisions\n",
        "\n",
        "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
        "- Which ngram dataset to use\n",
        "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
        "- Beam search parameters\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xb_twOmVsC6"
      },
      "source": [
        "## N-gram Dataset Selection\n",
        "We use Norvig's \"big.txt\" because it is a widely available, large, and diverse corpus. Its broad vocabulary and frequency information enable robust probability estimation for unigrams and, indirectly, for candidate generation.\n",
        "\n",
        "## Candidate Generation via Edits\n",
        " We generate candidates using one-edit (edits1) and, if needed, two-edits (edits2). This approach follows empirical evidence that most spelling mistakes are within one or two edit distances, and it relies on corpus frequencies to implicitly weight the candidates.\n",
        "\n",
        "## Edit Distance Penalty\n",
        "An edit distance penalty is applied to discourage corrections that require significant changes. This encourages minimal adjustments and balances the frequency-based probability with the similarity to the original word.\n",
        "\n",
        "## Beam Search for Sentence Correction\n",
        "A beam width of 5 is used for sentence-level correction to efficiently explore multiple candidate sequences while keeping computational costs manageable. This provides a balance between context sensitivity and runtime performance.\n",
        "\n",
        "## Dataset Parsing for Evaluation\n",
        "We parse error-marked sentences (e.g., <ERR targ=sister> siter </ERR>) to generate gold and noisy versions. This structured format facilitates accurate evaluation of the correction performance on realistic error data.\n",
        "\n",
        "## Speed Evaluation\n",
        "Measuring total and per-sentence correction times using Python’s time module ensures that our implementation is not only accurate but also efficient for practical applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46rk65S4GRSe"
      },
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity (or just take another dataset). Compare your solution to the Norvig's corrector, and report the accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "OwZWaX9VVs7B"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Corrector Evaluation ===\n",
            "Overall Word Accuracy: 70.00%\n",
            "Total Correction Time: 174.8377 sec\n",
            "Average Correction Time per Sentence: 0.5677 sec\n",
            "\n",
            "=== Baseline (Norvig) Corrector Evaluation ===\n",
            "Overall Word Accuracy: 70.24%\n",
            "Total Correction Time: 26.7063 sec\n",
            "Average Correction Time per Sentence: 0.0867 sec\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import time\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "class NorvigCorrector:\n",
        "    def __init__(self, corpus_path=\"big.txt\"):\n",
        "        with open(corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read().lower()\n",
        "        self.WORDS = Counter(re.findall(r'\\w+', text))\n",
        "        self.N = sum(self.WORDS.values())\n",
        "    \n",
        "    def P(self, word):\n",
        "        return self.WORDS[word] / self.N\n",
        "    \n",
        "    def correction(self, word):\n",
        "        return max(self.candidates(word), key=self.P)\n",
        "    \n",
        "    def candidates(self, word):\n",
        "        return (self.known([word]) or \n",
        "                self.known(self.edits1(word)) or \n",
        "                self.known(set(self.edits2(word))) or \n",
        "                {word})\n",
        "    \n",
        "    def known(self, words):\n",
        "        return set(w for w in words if w in self.WORDS)\n",
        "    \n",
        "    def edits1(self, word):\n",
        "        letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "        splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
        "        deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
        "        replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "        inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "        return set(deletes + transposes + replaces + inserts)\n",
        "    \n",
        "    def edits2(self, word):\n",
        "        return (e2 for e1 in self.edits1(word) for e2 in self.edits1(e1))\n",
        "    \n",
        "    def correct_sentence(self, sentence):\n",
        "        return \" \".join(self.correction(word) for word in sentence.split())\n",
        "    \n",
        "    def norvig_correct_sentence(self, sentence):\n",
        "        return self.correct_sentence(sentence)\n",
        "\n",
        "\n",
        "def parse_error_sentence(sentence):\n",
        "    \"\"\"\n",
        "    Parses a sentence with error markup of the form:\n",
        "       <ERR targ=correct_word> error_word </ERR>\n",
        "    \"\"\"\n",
        "    pattern = r\"<ERR\\s+targ=['\\\"]?([^\\\"'\\s>]+)['\\\"]?>(.*?)</ERR>\"\n",
        "    gold_sentence = re.sub(pattern, r\"\\1\", sentence)\n",
        "    noisy_sentence = re.sub(pattern, r\"\\2\", sentence)\n",
        "    return gold_sentence, noisy_sentence\n",
        "\n",
        "def load_dataset_from_file(file_path):\n",
        "    gold_sentences = []\n",
        "    noisy_sentences = []\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            gold, noisy = parse_error_sentence(line)\n",
        "            gold_sentences.append(gold)\n",
        "            noisy_sentences.append(noisy)\n",
        "    return gold_sentences, noisy_sentences\n",
        "\n",
        "\n",
        "def compute_word_accuracy(gold_sentence, corrected_sentence):\n",
        "    gold_tokens = gold_sentence.split()\n",
        "    corrected_tokens = corrected_sentence.split()\n",
        "    correct = sum(1 for g, c in zip(gold_tokens, corrected_tokens) if g == c)\n",
        "    return correct, len(gold_tokens)\n",
        "\n",
        "def evaluate_corrector(gold_sentences, noisy_sentences, corrector):\n",
        "    total_gold_words = 0\n",
        "    total_correct = 0\n",
        "    total_time = 0.0\n",
        "    detailed_results = []\n",
        "    \n",
        "    for gold, noisy in zip(gold_sentences, noisy_sentences):\n",
        "        start = time.time()\n",
        "        corrected = corrector.correct_sentence(noisy)\n",
        "        elapsed = time.time() - start\n",
        "        total_time += elapsed\n",
        "        \n",
        "        correct_count, total = compute_word_accuracy(gold, corrected)\n",
        "        total_correct += correct_count\n",
        "        total_gold_words += total\n",
        "        \n",
        "        detailed_results.append({\n",
        "            \"gold\": gold,\n",
        "            \"noisy\": noisy,\n",
        "            \"corrected\": corrected,\n",
        "            \"accuracy\": correct_count / total\n",
        "        })\n",
        "    \n",
        "    overall_accuracy = (total_correct / total_gold_words) * 100\n",
        "    avg_time = total_time / len(gold_sentences)\n",
        "    return overall_accuracy, total_time, avg_time, detailed_results\n",
        "\n",
        "\n",
        "dataset_file = \"eval.txt\"\n",
        "gold_sentences, noisy_sentences = load_dataset_from_file(dataset_file)\n",
        "\n",
        "corpus_file = \"big.txt\"\n",
        "fivegram_file = \"fivegrams.txt\"\n",
        "\n",
        "improved_corrector = SpellCorrector(corpus_path=corpus_file, fivegram_path=fivegram_file)\n",
        "baseline_corrector = NorvigCorrector(corpus_path=corpus_file)\n",
        "\n",
        "imp_accuracy, imp_total_time, imp_avg_time, imp_details = evaluate_corrector(gold_sentences, noisy_sentences, improved_corrector)\n",
        "\n",
        "base_accuracy, base_total_time, base_avg_time, base_details = evaluate_corrector(gold_sentences, noisy_sentences, baseline_corrector)\n",
        "\n",
        "print(\"=== Corrector Evaluation ===\")\n",
        "print(\"Overall Word Accuracy: {:.2f}%\".format(imp_accuracy))\n",
        "print(\"Total Correction Time: {:.4f} sec\".format(imp_total_time))\n",
        "print(\"Average Correction Time per Sentence: {:.4f} sec\".format(imp_avg_time))\n",
        "print(\"\\n=== Baseline (Norvig) Corrector Evaluation ===\")\n",
        "print(\"Overall Word Accuracy: {:.2f}%\".format(base_accuracy))\n",
        "print(\"Total Correction Time: {:.4f} sec\".format(base_total_time))\n",
        "print(\"Average Correction Time per Sentence: {:.4f} sec\".format(base_avg_time))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Useful resources (also included in the archive in moodle):\n",
        "\n",
        "1. [Possible dataset with N-grams](https://www.ngrams.info/download_coca.asp)\n",
        "2. [Damerau–Levenshtein distance](https://en.wikipedia.org/wiki/Damerau–Levenshtein_distance#:~:text=Informally%2C%20the%20Damerau–Levenshtein%20distance,one%20word%20into%20the%20other.)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
